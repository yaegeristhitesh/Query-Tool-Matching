{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s10eHfNuppXC",
        "outputId": "439a787e-7c78-49a6-e432-31fcc892294b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: tabulate in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (0.9.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: nltk in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: sentence_transformers in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (4.36.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (2.1.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (0.16.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (1.26.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.12.2)\n",
            "Requirement already satisfied: requests in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
            "Requirement already satisfied: click in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from torchvision->sentence_transformers) (10.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: textcortex in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (2.0.3)\n",
            "Requirement already satisfied: requests in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from textcortex) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from requests->textcortex) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from requests->textcortex) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from requests->textcortex) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (from requests->textcortex) (2023.11.17)\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: tabulate in c:\\users\\sahaj\\miniconda3\\lib\\site-packages (0.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install -U sentence_transformers\n",
        "!pip install spacy==3.7.2\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.2/en_core_web_trf-3.7.2.tar.gz --no-deps\n",
        "!pip install tabulate\n",
        "!pip install pandas\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnHgzewopw7w",
        "outputId": "2f354b49-1b14-4b1c-8499-b3605867abb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
            "[nltk_data]     connection attempt failed because the connected party\n",
            "[nltk_data]     did not properly respond after a period of time, or\n",
            "[nltk_data]     established connection failed because connected host\n",
            "[nltk_data]     has failed to respond>\n",
            "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
            "[nltk_data]     connection attempt failed because the connected party\n",
            "[nltk_data]     did not properly respond after a period of time, or\n",
            "[nltk_data]     established connection failed because connected host\n",
            "[nltk_data]     has failed to respond>\n",
            "c:\\Users\\sahaj\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
            "[nltk_data]     connection attempt failed because the connected party\n",
            "[nltk_data]     did not properly respond after a period of time, or\n",
            "[nltk_data]     established connection failed because connected host\n",
            "[nltk_data]     has failed to respond>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Updating the Tool Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To update the toolset, give information of new/updated tool in the following format given below and write in a json file. Pass the path of json file to the function ```update_toolset```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''{\n",
        "  <tool1_name>:{\n",
        "     \"Desription\":\" \",\n",
        "     \"ArgumentName\":{\n",
        "       <arg1_name>:{\n",
        "         \"Description\":\"\",\n",
        "         \"ArgumentType\": \"\"\n",
        "        },\n",
        "       <arg2_name>:{\n",
        "         \"Description\":\"\",\n",
        "         \"ArgumentType\": \"\"\n",
        "        }\n",
        "    }\n",
        "  },\n",
        "  <tool2_name>:{\n",
        "     \"Desription\":\" \",\n",
        "     \"ArgumentName\":{\n",
        "       <arg1_name>:{\n",
        "\n",
        "         \"Description\":\"\",\n",
        "         \"ArgumentType\": \"\"\n",
        "        },\n",
        "       <arg2_name>:{\n",
        "         \"Description\":\"\",\n",
        "         \"ArgumentType\": \"\"\n",
        "        }\n",
        "    }\n",
        "  }\n",
        "}'''\n",
        "\n",
        "def update_toolset(new_tools,existing_toolset='tools_dictionary.json'):\n",
        "  # new_tools : path to the json file containing new tools\n",
        "  # existing_toolset: path of the existing tool_dictionary.json file\n",
        "  with open(existing_toolset,\"r\") as f:\n",
        "    parent_file=json.load(f)\n",
        "\n",
        "\n",
        "  with open(new_tools,\"r\") as f1:\n",
        "    new_file=json.load(f1)\n",
        "\n",
        "  #adding the new tools\n",
        "  parent_file.update(new_file)\n",
        "\n",
        "  #making changes is the existing file only\n",
        "  with open(existing_toolset,\"w\") as fo:\n",
        "    json.dump(parent_file,fo,indent=4)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variable Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G-_LCpbbp3ya"
      },
      "outputs": [],
      "source": [
        "address = \"tools_dictionary.json\"\n",
        "with open(address, 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "NHYEjHVqp9I1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given a text, extracts actionable insights, and creates tasks for them, which are kind of a work item. - The text from which the actionable insights need to be created.\n",
            "Returns a list of work items that are similar to the given work item - The ID of the work item for which you want to find similar items\n",
            "Returns the ID of the current sprint - _\n",
            "Returns a list of objects sorted by priority. The logic of what constitutes priority for a given object is an internal implementation detail. - A list of objects to be prioritized\n",
            "Given a search string, returns the id of a matching object in the system of record. If multiple matches are found, it returns the one where the confidence is highest. - The search string, could be for example customerâ€™s name, part name, user name.\n",
            "Summarizes a list of objects. The logic of how to summarize a particular object type is an internal\n",
            "implementation\n",
            "detail. - List of objects to summarize\n",
            "Returns the ID of the current user - _\n",
            "Returns a list of work items matching the request - Filters for work belonging to any of the provided parts\n",
            "Returns a list of work items matching the request - Filters for work created by any of these users\n",
            "Returns a list of work items matching the request - Filters for issues with any of the provided priorities. Allowed values: p0, p1, p2,  p3\n",
            "Returns a list of work items matching the request - Filters for issues with any of the provided Rev organizations\n",
            "Returns a list of work items matching the request - The maximum number of works to return. The default is '50'\n",
            "Returns a list of work items matching the request - Filters for work owned by any of these users\n",
            "Returns a list of work items matching the request - Filters for records in the provided stage(s) by name\n",
            "Returns a list of work items matching the request - Filters for tickets that need a response\n",
            "Returns a list of work items matching the request - Filters for tickets associated with any of the provided Rev organizations\n",
            "Returns a list of work items matching the request - Filters for tickets with any of the provided severities. Allowed\n",
            "Returns a list of work items matching the request - values: blocker, high, low, medium\n",
            "Returns a list of work items matching the request - Filters for tickets with any of the provided source channels\n",
            "Returns a list of work items matching the request - Filters for work of the provided types. Allowed values: issue, ticket, task\n",
            "Adds the given work items to the sprint - A list of work item IDs to be added to the sprint.\n",
            "Adds the given work items to the sprint - The ID of the sprint to which the work items should be added\n"
          ]
        }
      ],
      "source": [
        "tools = []\n",
        "tool_descriptions = []\n",
        "arg_names = []\n",
        "arg_descriptions = []\n",
        "arg_to_tool_map = {}\n",
        "tool_enc = []\n",
        "tool_des_enc= []\n",
        "arg_enc = []\n",
        "arg_des_enc = []\n",
        "no_of_block = 5\n",
        "tool_embedding = []\n",
        "arg_embedding = []\n",
        "\n",
        "# Convert the dictionary to a list of descriptions without tool names\n",
        "descriptions = [\n",
        "    f\"{data[tool]['Description']} - {data[tool]['ArgumentName'][arg]['Description']}\"\n",
        "    for tool in data\n",
        "    for arg in data[tool]['ArgumentName']\n",
        "]\n",
        "\n",
        "# Print or use the list of concatenated descriptions\n",
        "for desc in descriptions:\n",
        "    print(desc)\n",
        "\n",
        "# Different Sentence Transformer Models we are using for matching with different subqueries\n",
        "models = [\n",
        "    'all-MiniLM-L6-v2'\n",
        "    ,'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
        "    ,'sentence-transformers/msmarco-MiniLM-L6-cos-v5'\n",
        "    ,'sentence-transformers/paraphrase-MiniLM-L3-v2'\n",
        "    ,'sentence-transformers/all-mpnet-base-v2'\n",
        "    # ,'sentence-transformers/all-MiniLM-L6-v2'\n",
        "    # ,'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'\n",
        "    # ,'sentence-transformers/paraphrase-albert-small-v2'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gEAmi487qHrA"
      },
      "outputs": [],
      "source": [
        "# Given a list of strings and Sentence Transformer model it returns a list of vector embeddings\n",
        "def embed_sentences(strings,model):\n",
        "        embeddings = []\n",
        "        for string in strings:\n",
        "            embeddings.append(np.array(model.encode(string), dtype=np.float32))\n",
        "        return np.array(embeddings)\n",
        "\n",
        "'''\n",
        "It extracts tool names, tool descriptions, argument names, argument descriptions\n",
        "and puts them in a list for each type and also generates a dictionary mapping\n",
        "each argument to its tool\n",
        "'''\n",
        "def extract():\n",
        "    tools.clear()\n",
        "    tool_descriptions.clear()\n",
        "    arg_names.clear()\n",
        "    arg_descriptions.clear()\n",
        "    arg_to_tool_map.clear()\n",
        "\n",
        "    for key, value in data.items():\n",
        "        tmp_tool = key\n",
        "        tools.append(tmp_tool)\n",
        "\n",
        "        tmp_tool_dsc = value.get(\"Description\")\n",
        "        tool_descriptions.append(tmp_tool_dsc)\n",
        "\n",
        "        tmp_arg_dsc = value.get(\"ArgumentName\")\n",
        "\n",
        "        if tmp_arg_dsc is not None:  # Check if tmp_arg_dsc is not None\n",
        "            for argName, argDesc in tmp_arg_dsc.items():\n",
        "                if argDesc.get(\"Description\") == \"_\":\n",
        "                    arg_names.append(key)\n",
        "                    arg_descriptions.append(tmp_tool_dsc)\n",
        "                    arg_to_tool_map[key] = key\n",
        "                else:\n",
        "                    arg_names.append(argName)\n",
        "                    arg_descriptions.append(argDesc.get(\"Description\"))\n",
        "                    arg_to_tool_map[argName] = key\n",
        "\n",
        "\n",
        "\n",
        "# Removes unnecessary stopwords like though, surely, etc. which could dilute the information in a sentence\n",
        "def remove_stopwords(query):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        word_tokens = word_tokenize(query)\n",
        "        filtered_query = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "        return ' '.join(filtered_query)\n",
        "\n",
        "# Generates embeddings\n",
        "def generate_embeddings(model):\n",
        "    tool_enc = embed_sentences(tools, model)\n",
        "    tool_des_enc = embed_sentences(tool_descriptions,model)\n",
        "    arg_enc = embed_sentences(arg_names,model)\n",
        "    arg_des_enc = embed_sentences(arg_descriptions,model)\n",
        "\n",
        "# Using spacy model to break the given query into multiple subqueries\n",
        "def break_down_complex_sentence(sentence):\n",
        "        # Load the spaCy model\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        doc = nlp(sentence)\n",
        "\n",
        "        # Initialize variables\n",
        "        clauses = []\n",
        "        current_clause = []\n",
        "\n",
        "        # Iterate through tokens in the sentence\n",
        "        for token in doc:\n",
        "            if token.dep_ in ['punct', 'cc'] and token.text != ',':\n",
        "                # If it's a punctuation or coordinating conjunction (cc) that is not a comma,\n",
        "                # consider it as the end of the current clause\n",
        "                if current_clause:\n",
        "                    clauses.append(' '.join(current_clause).strip())\n",
        "                    current_clause = []\n",
        "            elif token.text == ',':\n",
        "                # If it's a comma, consider it as the end of the current clause,\n",
        "                # and start a new clause if there are more tokens after the comma\n",
        "                if current_clause:\n",
        "                    clauses.append(' '.join(current_clause).strip())\n",
        "                    current_clause = []\n",
        "            else:\n",
        "                # Otherwise, add the token to the current clause\n",
        "                current_clause.append(token.text)\n",
        "\n",
        "        # Add the last clause if there is any\n",
        "        if current_clause:\n",
        "            clauses.append(' '.join(current_clause).strip())\n",
        "\n",
        "        return clauses\n",
        "\n",
        "def tokenize_sentence(sentence):\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    # Remove punctuation marks\n",
        "    words_without_punctuations = [word for word in words if word.isalnum()]\n",
        "\n",
        "    return words_without_punctuations\n",
        "    # Example usage\n",
        "    input_sentence = \"List all high severity tickets coming in from slack from customer Cust123 and generate a summary of them.\"\n",
        "    result = tokenize_sentence(input_sentence)\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "M8UXc8mt3VUl"
      },
      "outputs": [],
      "source": [
        "def calculate_embeddings(models):\n",
        "  extract()\n",
        "  tmodels =[]\n",
        "  tool_embedding.clear()\n",
        "  arg_embedding.clear()\n",
        "  for model in models:\n",
        "    tmodels.append(SentenceTransformer(model))\n",
        "  for model in tmodels:\n",
        "    temp = embed_sentences(tool_descriptions,model)\n",
        "    temp2 = embed_sentences(arg_descriptions, model)\n",
        "    tool_embedding.append(temp)\n",
        "    arg_embedding.append(temp2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mfCVARwbqsWh"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This function takes the query, list of Sentence Transformer models, and minimum threshold\n",
        "as arguments and implements our approach where it breaks down the query into multiple sub-queries\n",
        "and uses different models to compute sentence embeddings of both the list of sub-queries and list of\n",
        "tool descriptions. After calculating embeddings it generates a table of cosine similaritites\n",
        "where each row is assigned with a particular tool and each column is assigned with a particular\n",
        "subquery. From this table each column is analyzed to find which tool is giving maximum cosine similarity\n",
        "for a specific subquery that information is stored and this process is repeated over multiple Sentence\n",
        "Transformer models to calculate similar information. All this is consolidated into a final table where columns\n",
        "again correspond to subqueries and rows correspond to model and rows will contain best tools faoured according to\n",
        "the said model and from this table again a majority vote is taken to decide the final tools for each subquery.\n",
        "In future we would avoid collisions for those tools which will contain a single argument,i.e., if two subqueries\n",
        "are found be favouring same tool of said kind then the subquery demonastrating max cosine score with tool description\n",
        "will be selected while the second subquery will have to go with next most favoured tool.\n",
        "A threshold is provided to reject gibberish subqueries like \"What is the meaning of my life?\" which will produce a\n",
        "low cosine similarity with any tool on an average.\n",
        "'''\n",
        "def tool_computer(query, models, threshold):\n",
        "    sub_queries = break_down_complex_sentence(query)\n",
        "    #print(\"Sub-Queries\")\n",
        "    #print(sub_queries)\n",
        "    extract()\n",
        "    #print(\"Subqueries:\", sub_queries)\n",
        "    tool_des = tool_descriptions\n",
        "    #print(tool_des)\n",
        "    '''for i in range(len(tool_des)):\n",
        "      tool_des[i] = remove_stopwords(tool_des[i])\n",
        "    for i in range(len(sub_queries)):\n",
        "      sub_queries[i] = remove_stopwords(sub_queries[i])\n",
        "    '''\n",
        "    #print(tool_des)\n",
        "    # Create a 2D list\n",
        "\n",
        "    # Print the dimensions (number of rows and columns)\n",
        "    final_table = []\n",
        "    max_cs = None\n",
        "    done = False\n",
        "    for i, model in enumerate(models):\n",
        "        temp_model = SentenceTransformer(model)\n",
        "        table = np.zeros((len(tool_des),len(sub_queries)))\n",
        "        for x in range(table.shape[0]):\n",
        "            for y in range(table.shape[1]):\n",
        "                table[x][y] = util.cos_sim(temp_model.encode(tool_des[x]),temp_model.encode(sub_queries[y]))\n",
        "        if(done == False):\n",
        "            done = True\n",
        "            max_cs = np.max(table, axis=0)\n",
        "        else:\n",
        "            max_values = np.max(table, axis=0)\n",
        "            for i in range(max_values.shape[0]):\n",
        "                if(max_values[i] > max_cs[i]):\n",
        "                    max_cs[i] = max_values[i]\n",
        "        l = np.argmax(table, axis=0)\n",
        "        #print(\"Model name: \",model)\n",
        "        #print(\"Index of tools selected by it:\")\n",
        "        #print(l)\n",
        "        final_table.append(l)\n",
        "    #print(\"Final table:(From above information)\")\n",
        "    df = pd.DataFrame(final_table)\n",
        "    #print(df)\n",
        "    #stacked_array = np.vstack(final_table)\n",
        "\n",
        "    # Find the unique values and their counts column-wise\n",
        "    unique_values, counts = np.unique(final_table, axis=0, return_counts=True)\n",
        "\n",
        "    # Find the index of the maximum count for each column\n",
        "    max_count_indices = np.argmax(counts, axis=0)\n",
        "\n",
        "    # Get the values with maximum frequency for each column\n",
        "    max_freq_values = unique_values[max_count_indices]\n",
        "\n",
        "    # Create a list of indices to remove\n",
        "    indices_to_remove = [i for i, value in enumerate(max_cs) if value < threshold]\n",
        "\n",
        "    # Remove elements from max_cs based on indices\n",
        "    max_cs = [value for i, value in enumerate(max_cs) if i not in indices_to_remove]\n",
        "\n",
        "    # Remove corresponding elements from max_freq_values\n",
        "    max_freq_values = [value for i, value in enumerate(max_freq_values) if i not in indices_to_remove]\n",
        "\n",
        "\n",
        "    #print(\"Number with maximum frequency column-wise:\", max_freq_values)\n",
        "    #print(\"Maximum Thresholds observed column-wise:\",max_cs)\n",
        "    max_freq_desc = list(zip(max_freq_values,max_cs))\n",
        "    return max_freq_desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "def GetTools(query):\n",
        "    x = tool_computer(input, models, 0.4)\n",
        "\n",
        "    tool_list =[]\n",
        "    for element in x:\n",
        "        tool_list.append(tools[element[0]])\n",
        "\n",
        "    # Initialize lists to store argument details\n",
        "    argument_lists = []\n",
        "    argument_descriptions = []\n",
        "    argument_types = []\n",
        "\n",
        "    # Iterate through each tool in the tool list\n",
        "    for tool_name in tool_list:\n",
        "        if tool_name in data:\n",
        "            tool_data = data[tool_name][\"ArgumentName\"]\n",
        "            arguments = list(tool_data.keys())\n",
        "            descriptions = [tool_data[arg][\"Description\"] for arg in arguments]\n",
        "            types = [tool_data[arg][\"ArgumentType\"] for arg in arguments]\n",
        "\n",
        "            # Append to the respective lists\n",
        "            argument_lists.append(arguments)\n",
        "            argument_descriptions.append(descriptions)\n",
        "            argument_types.append(types)\n",
        "        else:\n",
        "            # If the tool is not found in the data dictionary\n",
        "            argument_lists.append([])\n",
        "            argument_descriptions.append([])\n",
        "            argument_types.append([])\n",
        "\n",
        "    # Set up the question-answering model\n",
        "    question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "    # Given data\n",
        "    sub_queries = break_down_complex_sentence(input)\n",
        "\n",
        "    argument_name = argument_lists\n",
        "    argument_description = argument_descriptions\n",
        "    argument_type = argument_types\n",
        "\n",
        "    # Function to generate context for a given tool and sub-query\n",
        "    def generate_context(tool, query, argument, argument_description, argument_type):\n",
        "        context = f\"{query} given tool used is {tool}.\"\n",
        "        question = f\"\"\"{{\n",
        "                        \"{tool}\": {{\n",
        "                            \"Description\": \"What is the argument value for the {argument} parameter in the {tool} tool?\",\n",
        "                            \"ArgumentValue\": {{\n",
        "                                \"{argument}\": {{\n",
        "                                    \"Description\": \"{argument_description}\",\n",
        "                                    \"ArgumentType\": \"{argument_type}\"\n",
        "                                }}\n",
        "                            }}\n",
        "                        }}\n",
        "                    }}\n",
        "                    \"\"\"\n",
        "        input_data = {\n",
        "            \"question\": question,\n",
        "            \"context\": context\n",
        "        }\n",
        "        result = question_answerer(input_data)\n",
        "        return result['answer'], result['score']\n",
        "\n",
        "    # Set threshold for filtering out answers\n",
        "    threshold = 0.1\n",
        "\n",
        "    # Lists to store filtered arguments with descriptions, types, and scores\n",
        "    filtered_arguments = []\n",
        "    filtered_argument_descriptions = []\n",
        "    filtered_argument_types = []\n",
        "\n",
        "    # Iterate through tools, sub_queries, and arguments\n",
        "    for tool, query in zip(tool_list, sub_queries):\n",
        "        # Find the index of the current tool\n",
        "        tool_index = tool_list.index(tool)\n",
        "\n",
        "        # Retrieve the corresponding argument information\n",
        "        argument_list = argument_name[tool_index]\n",
        "        arg_desc = argument_description[tool_index]\n",
        "        arg_type = argument_type[tool_index]\n",
        "\n",
        "        # Lists to store filtered sub-arguments for the current tool and query\n",
        "        filtered_sub_arguments = []\n",
        "        filtered_sub_arg_desc = []\n",
        "        filtered_sub_arg_type = []\n",
        "\n",
        "        # Iterate through sub-arguments if argument is a list\n",
        "        for sub_argument, desc, arg_type in zip(argument_list, arg_desc, arg_type):\n",
        "            answer, score = generate_context(tool, query, sub_argument, desc, arg_type)\n",
        "\n",
        "            # Filter out based on the threshold\n",
        "            if score >= threshold:\n",
        "                filtered_sub_arguments.append(sub_argument)\n",
        "                filtered_sub_arg_desc.append(desc)\n",
        "                filtered_sub_arg_type.append(arg_type)\n",
        "            else:\n",
        "                # Append empty strings if no arguments were found\n",
        "                filtered_sub_arguments.append(sub_argument)\n",
        "                filtered_sub_arg_desc.append(' ')\n",
        "                filtered_sub_arg_type.append(' ')\n",
        "\n",
        "        # Append the filtered sub-arguments for the current tool and query\n",
        "        if filtered_sub_arguments:\n",
        "            filtered_arguments.append(filtered_sub_arguments)\n",
        "            filtered_argument_descriptions.append(filtered_sub_arg_desc)\n",
        "            filtered_argument_types.append(filtered_sub_arg_type)\n",
        "        else:\n",
        "            # Append empty strings if no arguments were found\n",
        "            filtered_arguments.append(' ')\n",
        "            filtered_argument_descriptions.append(' ')\n",
        "            filtered_argument_types.append(' ')\n",
        "\n",
        "    # Given query and tools\n",
        "    arguments = filtered_arguments\n",
        "    argument_description = filtered_argument_descriptions\n",
        "    argument_types = filtered_argument_types\n",
        "\n",
        "    # Initialize the workflow\n",
        "    workflow = []\n",
        "\n",
        "    # Set the threshold\n",
        "    threshold = 0.3\n",
        "\n",
        "    # Initialize the prev_index\n",
        "    prev_index = -1\n",
        "\n",
        "    # Process each sub-query and build the workflow\n",
        "    for i, sub_query in enumerate(sub_queries):\n",
        "        if i < len(tool_list):  # Ensure that the index is within the range of the tools list\n",
        "            # Use the question-answering model to get the relevant information about the tool and its argument\n",
        "            tool_name = tool_list[i]\n",
        "            argument_names = arguments[i] if isinstance(arguments[i], list) else [arguments[i]]  # Handle the case where arguments is a string\n",
        "            tool_arguments = []\n",
        "\n",
        "            # Iterate through the sub-list of arguments\n",
        "            for argument_name, arg_desc, arg_type in zip(argument_names, argument_description[i], argument_types[i]):\n",
        "                # If the argument is empty, append an empty list to the workflow\n",
        "                if argument_name.strip() == '':\n",
        "                    tool_arguments = []  # Set tool_arguments to an empty list\n",
        "                    break  # Break out of the loop as no further processing is needed for empty arguments\n",
        "                else:\n",
        "                    # Modify the question and context for the question-answering model\n",
        "                    context = f\"\"\"\n",
        "                    {sub_query} given tool used {tool_name}\n",
        "                    \"\"\"\n",
        "\n",
        "                    question = f\"\"\"{{\n",
        "                        \"{tool_name}\": {{\n",
        "                            \"Description\": \"What is the argument value for the {argument_name} parameter in the {tool_name} tool?\",\n",
        "                            \"ArgumentValue\": {{\n",
        "                                \"{argument_name}\": {{\n",
        "                                    \"Description\": \"{arg_desc}\",\n",
        "                                    \"ArgumentType\": \"{arg_type}\"\n",
        "                                }}\n",
        "                            }}\n",
        "                        }}\n",
        "                    }}\n",
        "                    \"\"\"\n",
        "\n",
        "                    # Provide the modified question and context as a dictionary\n",
        "                    input_data = {\n",
        "                        \"question\": question,\n",
        "                        \"context\": context\n",
        "                    }\n",
        "\n",
        "                    # Get the answer using the question-answering model\n",
        "                    result = question_answerer(input_data)\n",
        "\n",
        "                    # Check the score and apply the threshold\n",
        "                    if result['score'] < threshold:\n",
        "                        # If the score is below the threshold, increment $$PREV[]\n",
        "                        prev_index += 1\n",
        "                        argument_value = \"$$PREV[{}]\".format(prev_index)\n",
        "                        tool_arguments.append({\"argument_name\": argument_name, \"argument_value\": argument_value})\n",
        "                    else:\n",
        "                        # If the score is above the threshold, use the answer from the model\n",
        "                        argument_value = result['answer']\n",
        "                        tool_arguments.append({\"argument_name\": argument_name, \"argument_value\": argument_value})\n",
        "\n",
        "            # Add the tool information with its arguments to the workflow\n",
        "            workflow.append({\"tool_name\": tool_name, \"arguments\": tool_arguments})\n",
        "\n",
        "\n",
        "    return workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run the test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKSB6wMM5lUz",
        "outputId": "ec1f2029-1c17-4698-ed07-3f1951f98f77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sahaj\\miniconda3\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.7.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "queries = [\n",
        "    'Get all work items similar to TKT-123, summarize them, create issues from that summary, and prioritize them',\n",
        "    'Given a customer meeting transcript T, create action items and add them to my current sprint',\n",
        "    'List all high severity tickets coming in from slack from customer Cust123 and generate a summary of them.',\n",
        "    'What are my all issues in the triage stage under part FEAT-123? Summarize them.',\n",
        "    'Summarize high severity tickets from the customer UltimateCustomer',\n",
        "    'Prioritize my P0 issues and add them to the current sprint',\n",
        "    'What is the meaning of life?',\n",
        "    'Summarize issues similar to don:core:dvrv-us-1:devo/0:issue/1'\n",
        "]\n",
        "outputs = []\n",
        "for input in queries:\n",
        "    outputs.append(GetTools(input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[{'tool_name': 'get_similar_work_items',\n",
              "   'arguments': [{'argument_name': 'work_id', 'argument_value': 'TKT-123'}]},\n",
              "  {'tool_name': 'summarize_objects',\n",
              "   'arguments': [{'argument_name': 'objects', 'argument_value': '$$PREV[0]'}]},\n",
              "  {'tool_name': 'summarize_objects',\n",
              "   'arguments': [{'argument_name': 'objects', 'argument_value': '$$PREV[1]'}]},\n",
              "  {'tool_name': 'prioritize_objects',\n",
              "   'arguments': [{'argument_name': 'objects',\n",
              "     'argument_value': '$$PREV[2]'}]}],\n",
              " [{'tool_name': 'create_actionable_tasks_from_text',\n",
              "   'arguments': [{'argument_name': 'text', 'argument_value': '$$PREV[0]'}]},\n",
              "  {'tool_name': 'add_work_items_to_sprint',\n",
              "   'arguments': [{'argument_name': 'work_ids', 'argument_value': '$$PREV[1]'},\n",
              "    {'argument_name': 'sprint_id', 'argument_value': '$$PREV[2]'}]}],\n",
              " [{'tool_name': 'summarize_objects',\n",
              "   'arguments': [{'argument_name': 'objects',\n",
              "     'argument_value': '$$PREV[0]'}]}],\n",
              " [{'tool_name': 'summarize_objects',\n",
              "   'arguments': [{'argument_name': 'objects',\n",
              "     'argument_value': '$$PREV[0]'}]}],\n",
              " [],\n",
              " [{'tool_name': 'add_work_items_to_sprint',\n",
              "   'arguments': [{'argument_name': 'work_ids', 'argument_value': '$$PREV[0]'},\n",
              "    {'argument_name': 'sprint_id', 'argument_value': '$$PREV[1]'}]}],\n",
              " [],\n",
              " [{'tool_name': 'create_actionable_tasks_from_text',\n",
              "   'arguments': [{'argument_name': 'text', 'argument_value': '$$PREV[0]'}]}]]"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do the evaluation, we are using BLEU Metric. To do meaning full evaluation, the format of the hypothesis and reference strings should be similar else the results are uninterpretable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Example 1: Query - Given a customer meeting transcript T, create action items and add them to my current sprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "hypothesis = \"\"\"\n",
        "[\n",
        "    {\n",
        "    \"tool_name\": \"create_actionable_tasks_from_text\",\n",
        "    \"arguments\": [\n",
        "            {\n",
        "    \"argument_name\": \"text\",\n",
        "    \"argument_value\": \"T\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "    \"tool_name\": \"get_sprint_id\",\n",
        "    \"arguments\": []\n",
        "    },\n",
        "    {\n",
        "    \"tool_name\": \"add_work_items_to_sprint\",\n",
        "    \"arguments\": [\n",
        "            {\n",
        "    \"argument_name\": \"work_ids\",\n",
        "    \"argument_value\": \"$$PREV[0]\"\n",
        "            },\n",
        "            {\n",
        "    \"argument_name\": \"sprint_id\",\n",
        "    \"argument_value\": \"$$PREV[1]\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\"\"\"\n",
        "reference = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"tool_name\": \"create_actionable_tasks_from_text\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"text\",\n",
        "        \"argument_value\": \"$$PREV[0]\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"add_work_items_to_sprint\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"work_ids\",\n",
        "        \"argument_value\": \"$$PREV[1]\"\n",
        "      },\n",
        "      {\n",
        "        \"argument_name\": \"sprint_id\",\n",
        "        \"argument_value\": \"$$PREV[2]\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The score for BLEU-5 is: 0.8101603728876992\n"
          ]
        }
      ],
      "source": [
        "BLEU_5 = sentence_bleu([reference], hypothesis, weights=(0.2, 0.2, 0.2, 0.2, 0.2))\n",
        "print(f\"\\nThe score for BLEU-5 is: {BLEU_5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Example 2: Query - Get all work items similar to TKT-123, summarize them, create issues from that summary, and prioritize them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "hypothesis = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"tool_name\": \"get_similar_work_items\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"work_id\",\n",
        "        \"argument_value\": \"TKT-123\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"summarize_objects\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"objects\",\n",
        "        \"argument_value\": \"$$PREV[0]\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"summarize_objects\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"objects\",\n",
        "        \"argument_value\": \"$$PREV[1]\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"prioritize_objects\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"objects\",\n",
        "        \"argument_value\": \"$$PREV[2]\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "reference = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"tool_name\": \"get_similar_work_items\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"work_id\",\n",
        "        \"argument_value\": \"TKT-123\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"summarize_objects\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"objects\",\n",
        "        \"argument_value\": \"$$PREV[0]\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"create_actionable_tasks_from_text\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"text\",\n",
        "        \"argument_value\": \"$$PREV[1]\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"prioritize_objects\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"objects\",\n",
        "        \"argument_value\": \"$$PREV[2]\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The score for BLEU-5 is: 0.9439792470039666\n"
          ]
        }
      ],
      "source": [
        "BLEU_5 = sentence_bleu([reference], hypothesis, weights=(0.2, 0.2, 0.2, 0.2, 0.2))\n",
        "print(f\"\\nThe score for BLEU-5 is: {BLEU_5}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Example 3: Query - List all high severity tickets coming in from slack from customer Cust123 and generate a summary of them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "hypothesis = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"tool_name\": \"summarize_objects\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"objects\",\n",
        "        \"argument_value\": \"$$PREV[0]\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "reference = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"tool_name\": \"search_object_by_name\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"query\",\n",
        "        \"argument_value\": \"Cust123\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"works_list\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"ticket.rev_org\",\n",
        "        \"argument_value\": [\n",
        "          \"$$PREV[0]\"\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        \"argument_name\": \"ticket.severity\",\n",
        "        \"argument_value\": [\n",
        "          \"high\"\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        \"argument_name\": \"ticket.source_channel\",\n",
        "        \"argument_value\": [\n",
        "          \"slack\"\n",
        "        ]\n",
        "      },\n",
        "      {\n",
        "        \"argument_name\": \"type\",\n",
        "        \"argument_value\": [\n",
        "          \"ticket\"\n",
        "        ]\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"tool_name\": \"summarize_objects\",\n",
        "    \"arguments\": [\n",
        "      {\n",
        "        \"argument_name\": \"objects\",\n",
        "        \"argument_value\": \"$$PREV[1]\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The score for BLEU-5 is: 0.015472752599220563\n"
          ]
        }
      ],
      "source": [
        "BLEU_5 = sentence_bleu([reference], hypothesis, weights=(0.2, 0.2, 0.2, 0.2, 0.2))\n",
        "print(f\"\\nThe score for BLEU-5 is: {BLEU_5}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
